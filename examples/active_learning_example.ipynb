{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning with Featuristic\n",
    "\n",
    "This notebook demonstrates how to use Featuristic in an active learning setup, where:\n",
    "\n",
    "1. We start with a very small labeled training set\n",
    "2. Train an initial model\n",
    "3. Use the model to identify the most uncertain examples from the unlabeled pool\n",
    "4. Request human labeling for those uncertain examples\n",
    "5. Add the newly labeled examples to the training set\n",
    "6. Retrain the model and observe the improvement in accuracy\n",
    "\n",
    "This approach minimizes human labeling effort by focusing it on the most informative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from featuristic import FeaturisticClassifier\n",
    "from featuristic import PromptFeatureDefinition, PromptFeatureConfiguration, extract_features\n",
    "from featuristic import Distribution\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup API Keys\n",
    "\n",
    "Configure your Azure OpenAI API key and endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables for Azure OpenAI\n",
    "AOAI_API_KEY = os.getenv(\"AOAI_API_KEY\")\n",
    "AOAI_API_ENDPOINT = os.getenv(\"AOAI_API_ENDPOINT\")\n",
    "\n",
    "if not AOAI_API_KEY or not AOAI_API_ENDPOINT:\n",
    "    print(\"Warning: Azure OpenAI credentials not found. Please set the environment variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load and prepare our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path) -> List[str]:\n",
    "    with open(path, \"r\") as f:\n",
    "        data = f.readlines()\n",
    "    return [json.loads(d)[\"text\"].strip().replace(\"\\n\\n\", \"\\n\") for d in data if \"text\" in json.loads(d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "russia_ukraine = load_data(\"data/russia_ukraine_2025.jsonl\")\n",
    "ones = np.ones(len(russia_ukraine))\n",
    "\n",
    "uk_us_relationship = load_data(\"data/uk_us_relationship.jsonl\")\n",
    "zeros = np.zeros(len(uk_us_relationship))\n",
    "\n",
    "X = np.array(russia_ukraine + uk_us_relationship) # cast to array for easier masking\n",
    "y = np.concatenate([ones, zeros])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Feature Configuration and Features\n",
    "\n",
    "Define the features we'll use for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature configuration\n",
    "config = PromptFeatureConfiguration(\n",
    "    aoai_api_key=AOAI_API_KEY,\n",
    "    aoai_api_endpoint=AOAI_API_ENDPOINT\n",
    ")\n",
    "\n",
    "# Helper function for text proportion features\n",
    "def as_propotion_of_text(x, text):\n",
    "    return x/len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features for classification\n",
    "mention_of_war = PromptFeatureDefinition(\n",
    "    name=\"mention_of_war\",\n",
    "    prompt=\"Whether or not the notion of war is mentioned\",\n",
    "    llm_return_type=bool,\n",
    "    distribution=Distribution.BERNOULLI,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "mention_of_casualties = PromptFeatureDefinition(\n",
    "    name=\"mention_of_casualties\",\n",
    "    prompt=\"Whether or not the notion of casualities are mentioned\",\n",
    "    llm_return_type=bool,\n",
    "    distribution=Distribution.BERNOULLI,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "mentions_weapons = PromptFeatureDefinition(\n",
    "    name=\"mentions_weapons\",\n",
    "    prompt=\"Whether or not the notion of weapons are mentioned\",\n",
    "    llm_return_type=bool,\n",
    "    distribution=Distribution.BERNOULLI,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "mentions_US = PromptFeatureDefinition(\n",
    "    name=\"mentions_US\",\n",
    "    prompt=\"A count of references to the United States\",\n",
    "    llm_return_type=int,\n",
    "    feature_post_callback=as_propotion_of_text,\n",
    "    distribution=Distribution.GAUSSIAN,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "mentions_Russia = PromptFeatureDefinition(\n",
    "    name=\"mentions_Russia\",\n",
    "    prompt=\"A count of references to Russians, Russia, or a place in Russia\",\n",
    "    llm_return_type=int,\n",
    "    feature_post_callback=as_propotion_of_text,\n",
    "    distribution=Distribution.GAUSSIAN,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "mentions_Ukraine = PromptFeatureDefinition(\n",
    "    name=\"mentions_Ukraine\",\n",
    "    prompt=\"A count of references to Ukrainians, Ukraine, or any place in Ukraine\",\n",
    "    llm_return_type=int,\n",
    "    feature_post_callback=as_propotion_of_text,\n",
    "    distribution=Distribution.GAUSSIAN,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "feature_definitions = [\n",
    "    mention_of_war,\n",
    "    mention_of_casualties,\n",
    "    mentions_weapons,\n",
    "    mentions_US,\n",
    "    mentions_Russia,\n",
    "    mentions_Ukraine\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"data/features.csv\"\n",
    "\n",
    "if os.path.exists(filepath):\n",
    "    # Load features from CSV if it exists\n",
    "    X_features_df = pd.read_csv(filepath)\n",
    "else:\n",
    "    X_features_df: pd.DataFrame = await extract_features(\n",
    "        X.tolist(),\n",
    "        feature_definitions)\n",
    "    \n",
    "    # Save features to CSV for later use\n",
    "    X_features_df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training/test datasets\n",
    "idx = np.arange(len(X))\n",
    "idx_train, idx_test = train_test_split(\n",
    "    idx, test_size=0.7, random_state=2, shuffle=True, stratify=y\n",
    ")\n",
    "\n",
    "data_train = X[idx_train]\n",
    "X_test = X[idx_test]\n",
    "\n",
    "y_train = y[idx_train]\n",
    "y_test = y[idx_test]\n",
    "\n",
    "features_train_df = X_features_df.iloc[idx_train]\n",
    "features_test_df = X_features_df.iloc[idx_test]\n",
    "\n",
    "# Start with just 2 examples from each class\n",
    "class_0_idx = np.where(y_train == 0)[0][[6, 15]]\n",
    "class_1_idx = np.where(y_train == 1)[0][[8, 1]]\n",
    "initial_idx = np.concatenate([class_0_idx, class_1_idx])\n",
    "\n",
    "is_labelled_mask = np.zeros(len(y_train), dtype=bool)\n",
    "is_labelled_mask[initial_idx] = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model Training and Evaluation\n",
    "\n",
    "Train a model on our very small initial training set and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate the model\n",
    "async def evaluate_model(classifier, features, y_true):\n",
    "    predictions = classifier.predict(features)\n",
    "    correct = np.sum(predictions == y_true)\n",
    "    accuracy = correct / len(y_true)\n",
    "    return accuracy, predictions, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See maximum accuracy with all training data\n",
    "\n",
    "featuristic_classifier = FeaturisticClassifier(\n",
    "    distributions=[d.distribution for d in feature_definitions]\n",
    ")\n",
    "\n",
    "featuristic_classifier.fit(\n",
    "    features_train_df,\n",
    "    y_train,\n",
    ")\n",
    "accuracy, predictions, features = await evaluate_model(\n",
    "    featuristic_classifier,\n",
    "    features_test_df,\n",
    "    y_test\n",
    ")\n",
    "print(f\"Accuracy with all training data: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuristic_classifier = FeaturisticClassifier(\n",
    "    distributions=[d.distribution for d in feature_definitions]\n",
    ")\n",
    "\n",
    "# Initial training and evaluation\n",
    "async def initial_training():    \n",
    "    print(\"Training initial classifier...\")\n",
    "    featuristic_classifier.fit(\n",
    "        features=features_train_df.iloc[is_labelled_mask],\n",
    "        Y=y_train[is_labelled_mask]\n",
    "    )\n",
    "    \n",
    "    print(\"Evaluating on test set...\")\n",
    "    test_accuracy, _, _ = await evaluate_model(featuristic_classifier, features_test_df, y_test)\n",
    "    \n",
    "    print(f\"Initial model accuracy on test set with 2 examples per class: {test_accuracy:.2f}\")\n",
    "    \n",
    "    return test_accuracy\n",
    "\n",
    "# Run initial training\n",
    "initial_accuracy = await initial_training()\n",
    "\n",
    "# Store accuracy history for tracking progress\n",
    "accuracy_history = [initial_accuracy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning Loop\n",
    "\n",
    "Now we'll implement the active learning loop with human-in-the-loop labeling. In each iteration:\n",
    "\n",
    "1. Extract features for unlabeled examples\n",
    "2. Rank examples by uncertainty\n",
    "3. Present the most uncertain example for human labeling\n",
    "4. Add the newly labeled example to the training set\n",
    "5. Retrain the model\n",
    "6. Evaluate the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Active learning iterations with human-in-the-loop labeling\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "async def active_learning_iteration(i, \n",
    "                                    data_train: List[str],\n",
    "                                    features_train_df: pd.DataFrame,\n",
    "                                    y_train,\n",
    "                                    is_labelled_mask: np.ndarray,\n",
    "                                    featuristic_classifier: FeaturisticClassifier,\n",
    "                                    auto_label: bool = False,\n",
    "                                    ):\n",
    "    print(f\"\\n=== Active Learning Iteration {i} ===\")\n",
    "    \n",
    "    # Rank examples by uncertainty\n",
    "    uncertainty_ranks = featuristic_classifier.rank_by_uncertainty(features_train_df.iloc[~is_labelled_mask])\n",
    "    # Get the most uncertain example\n",
    "    most_uncertain_idx = uncertainty_ranks[0]\n",
    "    most_uncertain_text = data_train[np.where(~is_labelled_mask)[0][most_uncertain_idx]]\n",
    "    \n",
    "    # Display the text for human labeling\n",
    "    print(f\"\\nPlease label this example:\")\n",
    "    print(f\"\\n{most_uncertain_text.strip()}...\\n\")\n",
    "\n",
    "    if auto_label:\n",
    "        # Use ground truth labels for labeling\n",
    "        print(\"Auto-labeling enabled. Using model prediction.\")\n",
    "        human_label = int(y_train[np.where(~is_labelled_mask)[0][most_uncertain_idx]])\n",
    "    else:\n",
    "        # Get human label through input\n",
    "        sleep(0.5)\n",
    "        label_valid = False\n",
    "        while not label_valid:\n",
    "            human_input = input(f\"Is this about Russia-Ukraine (1) or UK-US relationship (0)? Type 1 or 0 (Expected: {int(y_train[np.where(~is_labelled_mask)[0][most_uncertain_idx]])}): \")\n",
    "            try:\n",
    "                human_label = int(human_input)\n",
    "                if human_label in [0, 1]:\n",
    "                    label_valid = True\n",
    "                else:\n",
    "                    print(\"Please enter either 0 or 1.\")\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter a number (0 or 1).\")\n",
    "    \n",
    "    print(f\"You labeled this as: {'Russia-Ukraine' if human_label == 1 else 'UK-US Relationship'}\")\n",
    "    \n",
    "    # Update the label in case the user provided a different one\n",
    "    y_train[np.where(~is_labelled_mask)[0][most_uncertain_idx]] = human_label\n",
    "    # Update the mask to include the newly labeled example\n",
    "    is_labelled_mask[np.where(~is_labelled_mask)[0][most_uncertain_idx]] = True\n",
    "\n",
    "\n",
    "    # Retrain the classifier with the new data\n",
    "    print(\"Retraining classifier...\")\n",
    "    featuristic_classifier = FeaturisticClassifier(\n",
    "        distributions=[d.distribution for d in feature_definitions]\n",
    "    )\n",
    "    featuristic_classifier.fit(features=features_train_df.iloc[is_labelled_mask], Y=y_train[is_labelled_mask])\n",
    "    \n",
    "    # Evaluate the model\n",
    "    acc, _, _ = await evaluate_model(featuristic_classifier, features_test_df, y_test)\n",
    "    print(f\"Model accuracy after iteration {i}: {acc:.2f}\")\n",
    "    \n",
    "    return is_labelled_mask, y_train, acc, featuristic_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform active learning for a set number of iterations\n",
    "num_iterations = 50  # Keeping it small for the example\n",
    "auto_label = True  # Set to True for automatic labeling\n",
    "\n",
    "# Take copies to enable re-running\n",
    "is_labelled_mask_temp = is_labelled_mask.copy()\n",
    "y_train_temp = y_train.copy()\n",
    "\n",
    "\n",
    "for i in range(1, num_iterations + 1):\n",
    "    is_labelled_mask_temp, y_train_temp, acc, featuristic_classifier = await active_learning_iteration(\n",
    "            i, \n",
    "            features_train_df=features_train_df, \n",
    "            is_labelled_mask=is_labelled_mask_temp,\n",
    "            data_train=data_train,\n",
    "            y_train=y_train_temp,\n",
    "            featuristic_classifier=featuristic_classifier,\n",
    "            auto_label=auto_label\n",
    "        )\n",
    "    \n",
    "    accuracy_history.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final accuracy history\n",
    "print(\"\\nFinal accuracy history:\")\n",
    "for i, acc in enumerate(accuracy_history):\n",
    "    print(f\"Iteration {i}: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def random_selection_iteration(i, \n",
    "                               data_train: List[str],\n",
    "                               features_train_df: pd.DataFrame,\n",
    "                               y_train,\n",
    "                               is_labelled_mask: np.ndarray,\n",
    "                               featuristic_classifier: FeaturisticClassifier,\n",
    "                               auto_label: bool = False):\n",
    "    print(f\"\\n=== Active Learning Iteration {i} ===\")\n",
    "    \n",
    "    # Get a random example from the unlabelled data\n",
    "    random_idx = np.random.choice(np.where(~is_labelled_mask)[0], 1)[0]\n",
    "    random_text = data_train[random_idx]\n",
    "    \n",
    "    # Display the text for human labeling\n",
    "    print(f\"\\nPlease label this example:\")\n",
    "    print(f\"\\n{random_text.strip()}...\\n\")\n",
    "\n",
    "    if auto_label:\n",
    "        # Use ground truth labels for labeling\n",
    "        print(\"Auto-labeling enabled. Using model prediction.\")\n",
    "        human_label = int(y_train[random_idx])\n",
    "    else:\n",
    "        # Get human label through input\n",
    "        sleep(0.5)\n",
    "        label_valid = False\n",
    "        while not label_valid:\n",
    "            human_input = input(f\"Is this about Russia-Ukraine (1) or UK-US relationship (0)? Type 1 or 0 (Expected: {int(y_train[random_idx])}): \")\n",
    "            try:\n",
    "                human_label = int(human_input)\n",
    "                if human_label in [0, 1]:\n",
    "                    label_valid = True\n",
    "                else:\n",
    "                    print(\"Please enter either 0 or 1.\")\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter a number (0 or 1).\")\n",
    "    \n",
    "    print(f\"You labeled this as: {'Russia-Ukraine' if human_label == 1 else 'UK-US Relationship'}\")\n",
    "    \n",
    "    # Update the label in case the user provided a different one\n",
    "    y_train[random_idx] = human_label\n",
    "    # Update the mask to include the newly labeled example\n",
    "    is_labelled_mask[random_idx] = True\n",
    "\n",
    "\n",
    "    # Retrain the classifier with the new data\n",
    "    print(\"Retraining classifier...\")\n",
    "    featuristic_classifier = FeaturisticClassifier(\n",
    "        distributions=[d.distribution for d in feature_definitions]\n",
    "    )\n",
    "    featuristic_classifier.fit(features=features_train_df.iloc[is_labelled_mask], Y=y_train[is_labelled_mask])\n",
    "    \n",
    "    # Evaluate the model\n",
    "    acc, _, _ = await evaluate_model(featuristic_classifier, features_test_df, y_test)\n",
    "    print(f\"Model accuracy after iteration {i}: {acc:.2f}\")\n",
    "    \n",
    "    return is_labelled_mask, y_train, acc, featuristic_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_accuracy = await initial_training()\n",
    "accuracy_history = [initial_accuracy]\n",
    "\n",
    "# Perform active learning for a set number of iterations\n",
    "num_iterations = 50  # Keeping it small for the example\n",
    "auto_label = True  # Set to True for automatic labeling\n",
    "\n",
    "# Take copies to enable re-running\n",
    "is_labelled_mask_temp = is_labelled_mask.copy()\n",
    "y_train_temp = y_train.copy()\n",
    "\n",
    "\n",
    "for i in range(1, num_iterations + 1):\n",
    "    is_labelled_mask_temp, y_train_temp, acc, featuristic_classifier = await random_selection_iteration(\n",
    "            i, \n",
    "            features_train_df=features_train_df, \n",
    "            is_labelled_mask=is_labelled_mask_temp,\n",
    "            data_train=data_train,\n",
    "            y_train=y_train_temp,\n",
    "            featuristic_classifier=featuristic_classifier,\n",
    "            auto_label=auto_label\n",
    "        )\n",
    "    \n",
    "    accuracy_history.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final accuracy history\n",
    "print(\"\\nFinal accuracy history:\")\n",
    "for i, acc in enumerate(accuracy_history):\n",
    "    print(f\"Iteration {i}: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
